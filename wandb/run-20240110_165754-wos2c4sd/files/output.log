
  0%|                                                                                                                             | 0/200 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
















 15%|█████████████████▍                                                                                                  | 30/200 [00:37<03:27,  1.22s/it]Traceback (most recent call last):
  File "train.py", line 222, in <module>
    supervised_finetuning_trainer.train()
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/trainer.py", line 2735, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/trainer.py", line 2758, in compute_loss
    outputs = model(**inputs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/utils/operations.py", line 659, in forward
    return model_forward(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/utils/operations.py", line 647, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/peft/peft_model.py", line 536, in forward
    return self.get_base_model()(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1053, in forward
    outputs = self.model(
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 938, in forward
    layer_outputs = decoder_layer(
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 676, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 177, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/bitsandbytes/nn/modules.py", line 248, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)