
  0%|                                                                                                                             | 0/200 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


















 25%|█████████████████████████████                                                                                       | 50/200 [00:40<01:33,  1.61it/s]


















 49%|████████████████████████████████████████████████████████▊                                                           | 98/200 [01:15<01:21,  1.25it/s]



















 74%|█████████████████████████████████████████████████████████████████████████████████████                              | 148/200 [01:53<00:34,  1.51it/s]

 75%|██████████████████████████████████████████████████████████████████████████████████████▎                            | 150/200 [01:55<00:34,  1.45it/s]Checkpoint destination directory ./save/SFT/Mistral-7B-Instruct-v0.1/checkpoint-150 already exists and is non-empty.Saving will proceed but saved results may be invalid.

















100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 199/200 [02:31<00:00,  1.33it/s]
{'loss': 0.3754, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.03}

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [02:32<00:00,  1.31it/s]