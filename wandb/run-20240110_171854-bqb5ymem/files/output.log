
  0%|                                                                                                                             | 0/200 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

















 24%|████████████████████████████▍                                                                                       | 49/200 [00:37<01:47,  1.41it/s]


















 49%|████████████████████████████████████████████████████████▊                                                           | 98/200 [01:13<01:15,  1.36it/s]


















 74%|████████████████████████████████████████████████████████████████████████████████████▌                              | 147/200 [01:49<00:41,  1.29it/s]

 75%|██████████████████████████████████████████████████████████████████████████████████████▎                            | 150/200 [01:51<00:35,  1.40it/s]Checkpoint destination directory ./save/SFT/Mistral-7B-Instruct-v0.1/checkpoint-150 already exists and is non-empty.Saving will proceed but saved results may be invalid.

















 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 198/200 [02:28<00:01,  1.33it/s]
{'loss': 0.3761, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.03}

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [02:29<00:00,  1.33it/s]