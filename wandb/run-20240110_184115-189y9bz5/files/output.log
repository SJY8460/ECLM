
  0%|                                                                                                                             | 0/200 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


















 25%|█████████████████████████████                                                                                       | 50/200 [00:39<01:43,  1.45it/s]



















 50%|█████████████████████████████████████████████████████████▍                                                          | 99/200 [01:18<01:22,  1.23it/s]



















 74%|█████████████████████████████████████████████████████████████████████████████████████▋                             | 149/200 [01:56<00:36,  1.38it/s]





















100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [02:38<00:00,  1.26it/s]
{'loss': 0.3755, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.03}
{'train_runtime': 163.0686, 'train_samples_per_second': 2.453, 'train_steps_per_second': 1.226, 'train_loss': 0.5675075435638428, 'epoch': 0.03}