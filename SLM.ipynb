{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-09 22:57:13,566] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig\n",
    "from langchain import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(file_path, is_train=False):\n",
    "    \"\"\"Read data file of given path.\n",
    "\n",
    "    :param file_path: path of data file.\n",
    "    :param is_train: flag to indicate if it's a training file.\n",
    "    :return: list of sentence, list of slot and list of intent.\n",
    "    \"\"\"\n",
    "    texts, slots, intents, token_intents = [], [], [], []\n",
    "    text, slot, token_intent = [], [], []\n",
    "\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            items = line.strip().split()\n",
    "\n",
    "            if len(items) == 1:\n",
    "                texts.append(' '.join(text))\n",
    "                slots.append(slot)\n",
    "                if is_train:\n",
    "                    token_intents.append(token_intent)\n",
    "                if \"/\" not in items[0]:\n",
    "                    intents.append(items[0])\n",
    "                else:\n",
    "                    new = items[0].split(\"/\")\n",
    "                    intents.append(new[1])\n",
    "\n",
    "                # clear buffer lists.\n",
    "                text, slot, token_intent = [], [], []\n",
    "\n",
    "            elif len(items) >= 2:\n",
    "                text.append(items[0].strip())\n",
    "                slot.append(items[1].strip())\n",
    "                if is_train:\n",
    "                    token_intent.append(items[2].strip())\n",
    "\n",
    "    if is_train:\n",
    "        return texts, slots, intents, token_intents\n",
    "    else:\n",
    "        return texts, slots, intents\n",
    "    \n",
    "def format_data(texts, slots, intents, token_intents=None):\n",
    "    formatted_data = []\n",
    "\n",
    "    for text, slot, intent, token_intent in zip(texts, slots, intents, token_intents if token_intents else [None] * len(texts)):\n",
    "        words = text.split()\n",
    "        slot_dict = {}\n",
    "        sub_utterance_dict = {}\n",
    "        current_slot = None\n",
    "        current_value = []\n",
    "        current_sub = []\n",
    "        current_intent = None\n",
    "\n",
    "        # 处理slots，转换为字典格式\n",
    "        for word, slot_type in zip(words, slot):\n",
    "            if slot_type.startswith(\"B-\"):\n",
    "                if current_slot and current_value:\n",
    "                    slot_dict[current_slot] = ' '.join(current_value)\n",
    "                current_slot = slot_type[2:]\n",
    "                current_value = [word]\n",
    "            elif slot_type.startswith(\"I-\") and current_slot == slot_type[2:]:\n",
    "                current_value.append(word)\n",
    "            else:\n",
    "                if current_slot and current_value:\n",
    "                    slot_dict[current_slot] = ' '.join(current_value)\n",
    "                current_slot = None\n",
    "                current_value = []\n",
    "\n",
    "        if current_slot and current_value:\n",
    "            slot_dict[current_slot] = ' '.join(current_value)\n",
    "\n",
    "        # 处理token_intents，生成子句字典\n",
    "        if token_intent:\n",
    "            for word, ti in zip(words, token_intent):\n",
    "                if ti != \"SEP\":\n",
    "                    current_sub.append(word)\n",
    "                    current_intent = ti\n",
    "                else:\n",
    "                    if current_sub and current_intent:\n",
    "                        sub_utterance_dict[current_intent] = ' '.join(current_sub)\n",
    "                    current_sub = []\n",
    "                    current_intent = None\n",
    "\n",
    "            if current_sub and current_intent:\n",
    "                sub_utterance_dict[current_intent] = ' '.join(current_sub)\n",
    "        else:\n",
    "            sub_utterance_dict = None\n",
    "\n",
    "        formatted_example = {\n",
    "            'utterance': text,\n",
    "            'sub_utterance': sub_utterance_dict,\n",
    "            'intent(s)': intent,\n",
    "            'slots': ' '.join(slot),\n",
    "            'entity_slots': slot_dict\n",
    "        }\n",
    "        formatted_data.append(formatted_example)\n",
    "\n",
    "    return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sub_utterance', 'intent(s)', 'slots', 'utterance', 'entity_slots'],\n",
      "        num_rows: 13162\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sub_utterance', 'intent(s)', 'slots', 'utterance', 'entity_slots'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sub_utterance', 'intent(s)', 'slots', 'utterance', 'entity_slots'],\n",
      "        num_rows: 828\n",
      "    })\n",
      "})\n",
      "13162\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载训练、验证和测试数据\n",
    "data_files = {\n",
    "    \"train\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/train.json\",\n",
    "    \"validation\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/dev.json\",\n",
    "    \"test\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/test.json\"\n",
    "}\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = load_dataset('json', data_files=data_files['train'])\n",
    "dev_dataset =  load_dataset('json', data_files=data_files['validation'])\n",
    "test_dataset =  load_dataset('json', data_files=data_files['test'])\n",
    "# 查看数据集结构\n",
    "print(train_dataset,dev_dataset,test_dataset)\n",
    "print(len(train_dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Instruction]\n",
      "you are an expert of spoken language understanding, I need you to perform intent detection and slot filling for given utterance. \n",
      "\n",
      "\n",
      "[Input]\n",
      "utterance: define airline ua , names of airports and also show me city served both by nationair and canadian airlines international \n",
      "\n",
      "[Response]\n",
      "intent: atis_abbreviation#atis_airport#atis_city\n",
      "entity_slot: {'airline_code': 'ua', 'airline_name': 'canadian airlines international'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_template = \"\"\"\n",
    "[Instruction]\n",
    "you are an expert of spoken language understanding, I need you to perform intent detection and slot filling for given utterance. \\n\n",
    "\n",
    "[Input]\n",
    "utterance: {utterance} \n",
    "\n",
    "[Response]\n",
    "intent: {intent}\n",
    "entity_slot: {entity_slots}\n",
    "\"\"\"\n",
    "\n",
    "test_template =  \"\"\"\n",
    "[Instruction]\n",
    "you are an expert of spoken language understanding, I need you to perform intent detection and slot filling for given utterance. \\n\n",
    "\n",
    "[Input]\n",
    "utterance: {utterance} \n",
    "\n",
    "[Response]\n",
    "intent: {intent}\n",
    "entity_slot: {entity_slots}\n",
    "\"\"\"\n",
    "\n",
    "train_prompt = PromptTemplate(template=train_template, input_variables=['utterance'  'intent' 'entity_slots'])\n",
    "test_prompt = PromptTemplate(template=test_template, input_variables=['utterance' 'intent' 'entity_slots'])\n",
    "\n",
    "def format_text(example, is_train=True):\n",
    "    if is_train:\n",
    "        return train_prompt.format(utterance=example['utterance'], \n",
    "                                #    sub_sentence={k: v for k, v in example['sub_utterance'].items() if v is not None},\n",
    "                                   intent=example['intent(s)'],\n",
    "                                   entity_slots={k: v for k, v in example['entity_slots'].items() if v is not None})\n",
    "    else:\n",
    "        return test_prompt.format(utterance=example['utterance'], \n",
    "                                  intent=example['intent(s)'],\n",
    "                                  entity_slots={k: v for k, v in example['entity_slots'].items() if v is not None})\n",
    "\n",
    "# 应用format_text到数据集\n",
    "train_dataset = train_dataset.map(lambda x: {\"formatted_text\": format_text(x, is_train=True)})\n",
    "dev_dataset = dev_dataset.map(lambda x: {\"formatted_text\": format_text(x, is_train=False)})\n",
    "test_dataset = test_dataset.map(lambda x: {\"formatted_text\": format_text(x, is_train=False)})\n",
    "\n",
    "# 查看处理后的数据集\n",
    "print(train_dataset['train']['formatted_text'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/shangjian/code/Research/Multimodal & LLM/dataroot/models/Mistral/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c2b8ba09954cf1972db586904f05c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs= 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./save/SFT/{}\".format(model_id.split('/')[-1]) , \n",
    "    per_device_train_batch_size=bs,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    # num_train_epochs= 1,\n",
    "    logging_strategy=\"steps\",\n",
    "    # max_steps=int(len(train_dataset['train'])/ bs),\n",
    "    max_steps=200,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    run_name=\"baseline-{}\".format(model_id.split('/')[-1]),\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "supervised_finetuning_trainer = SFTTrainer(\n",
    "    base_model,\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=qlora_config,\n",
    "    dataset_text_field=\"formatted_text\",\n",
    "    max_seq_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m520\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d06b830d11c48cfb247a64a5aa6fe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111259324890044, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shangjian/code/Research/Multimodal & LLM/SLM/wandb/run-20240109_225809-rtphwter</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/520/huggingface/runs/rtphwter' target=\"_blank\">baseline-Mistral-7B-Instruct-v0.1</a></strong> to <a href='https://wandb.ai/520/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/520/huggingface' target=\"_blank\">https://wandb.ai/520/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/520/huggingface/runs/rtphwter' target=\"_blank\">https://wandb.ai/520/huggingface/runs/rtphwter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.446300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./save/SFT/Mistral-7B-Instruct-v0.1/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.6277337551116944, metrics={'train_runtime': 87.2992, 'train_samples_per_second': 2.291, 'train_steps_per_second': 2.291, 'total_flos': 1319195677040640.0, 'train_loss': 0.6277337551116944, 'epoch': 0.02})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_finetuning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/shangjian/code/Research/Multimodal & LLM/SLM/save/model/\" + model_id.split('/')[-1] \n",
    "supervised_finetuning_trainer.save_model(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_generated_text(generated_text):\n",
    "    # 使用正则表达式匹配意图和实体槽位\n",
    "    intent_pattern = r\"intent: ([\\w#]+)\"\n",
    "    entity_slots_pattern = r\"entity_slot: \\{([^}]+)\\}\"\n",
    "    utterance_pattern = r\"utterance: (.+)\"\n",
    "\n",
    "    # 提取意图\n",
    "    intent_match = re.search(intent_pattern, generated_text)\n",
    "    intents = intent_match.group(1).split('#') if intent_match else []\n",
    "\n",
    "    # 提取实体槽位\n",
    "    entity_slots_match = re.search(entity_slots_pattern, generated_text)\n",
    "    entity_slots = {}\n",
    "    if entity_slots_match:\n",
    "        slots_str = entity_slots_match.group(1)\n",
    "        for slot_str in slots_str.split(', '):\n",
    "            if ':' in slot_str:\n",
    "                key, value = slot_str.split(': ')\n",
    "                entity_slots[key.strip(\"'\")] = value.strip(\"'\")\n",
    "\n",
    "    # 提取utterance\n",
    "    utterance_match = re.search(utterance_pattern, generated_text)\n",
    "    utterance = utterance_match.group(1).strip() if utterance_match else \"\"\n",
    "\n",
    "    return intents, entity_slots, utterance\n",
    "\n",
    "def convert_dict_to_slots(entity_slots, sentence):\n",
    "    words = sentence.split()\n",
    "    slot_sequence = ['O'] * len(words)  # 初始化槽位序列为全'O'\n",
    "\n",
    "    for slot_type, slot_value in entity_slots.items():\n",
    "        if slot_value:\n",
    "            slot_words = slot_value.split()\n",
    "            start_index = find_sublist_index(slot_words, words)\n",
    "\n",
    "            if start_index != -1:\n",
    "                # 标记B类型槽位\n",
    "                slot_sequence[start_index] = f\"B-{slot_type}\"\n",
    "                # 标记随后的I类型槽位\n",
    "                for i in range(start_index + 1, start_index + len(slot_words)):\n",
    "                    slot_sequence[i] = f\"I-{slot_type}\"\n",
    "\n",
    "    return slot_sequence\n",
    "\n",
    "def find_sublist_index(sublist, lst):\n",
    "    for i in range(len(lst) - len(sublist) + 1):\n",
    "        if sublist == lst[i:i + len(sublist)]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def get_multi_acc(pred_output, golds):\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    for p, c in zip(pred_output, golds):\n",
    "        # print(p ,'<=>', c , c == p)\n",
    "        if set(p) == set(c):\n",
    "            acc += 1\n",
    "        total += 1\n",
    "    return acc / total\n",
    "\n",
    "\n",
    "# compute f1 score is modified from conlleval.pl\n",
    "def __startOfChunk(prevTag, tag, prevTagType, tagType, chunkStart=False):\n",
    "    if prevTag == 'B' and tag == 'B':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'I' and tag == 'B':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'O' and tag == 'B':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'O' and tag == 'I':\n",
    "        chunkStart = True\n",
    "\n",
    "    if prevTag == 'E' and tag == 'E':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'E' and tag == 'I':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'O' and tag == 'E':\n",
    "        chunkStart = True\n",
    "    if prevTag == 'O' and tag == 'I':\n",
    "        chunkStart = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prevTagType != tagType:\n",
    "        chunkStart = True\n",
    "    return chunkStart\n",
    "\n",
    "\n",
    "def __endOfChunk(prevTag, tag, prevTagType, tagType, chunkEnd=False):\n",
    "    if prevTag == 'B' and tag == 'B':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'B' and tag == 'O':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'I' and tag == 'B':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'I' and tag == 'O':\n",
    "        chunkEnd = True\n",
    "\n",
    "    if prevTag == 'E' and tag == 'E':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'E' and tag == 'I':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'E' and tag == 'O':\n",
    "        chunkEnd = True\n",
    "    if prevTag == 'I' and tag == 'O':\n",
    "        chunkEnd = True\n",
    "\n",
    "    if prevTag != 'O' and prevTag != '.' and prevTagType != tagType:\n",
    "        chunkEnd = True\n",
    "    return chunkEnd\n",
    "\n",
    "\n",
    "def __splitTagType(tag):\n",
    "    s = tag.split('-')\n",
    "    if len(s) > 2 or len(s) == 0:\n",
    "        raise ValueError('tag format wrong. it must be B-xxx.xxx')\n",
    "    if len(s) == 1:\n",
    "        tag = s[0]\n",
    "        tagType = \"\"\n",
    "    else:\n",
    "        tag = s[0]\n",
    "        tagType = s[1]\n",
    "    return tag, tagType\n",
    "\n",
    "\n",
    "def computeF1Score(correct_slots, pred_slots):\n",
    "    correctChunk = {}\n",
    "    correctChunkCnt = 0.0\n",
    "    foundCorrect = {}\n",
    "    foundCorrectCnt = 0.0\n",
    "    foundPred = {}\n",
    "    foundPredCnt = 0.0\n",
    "    correctTags = 0.0\n",
    "    tokenCount = 0.0\n",
    "    for correct_slot, pred_slot in zip(correct_slots, pred_slots):\n",
    "        inCorrect = False\n",
    "        lastCorrectTag = 'O'\n",
    "        lastCorrectType = ''\n",
    "        lastPredTag = 'O'\n",
    "        lastPredType = ''\n",
    "        for c, p in zip(correct_slot, pred_slot):\n",
    "            correctTag, correctType = __splitTagType(c)\n",
    "            predTag, predType = __splitTagType(p)\n",
    "\n",
    "            if inCorrect == True:\n",
    "                if __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
    "                 __endOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
    "                 (lastCorrectType == lastPredType):\n",
    "                    inCorrect = False\n",
    "                    correctChunkCnt += 1.0\n",
    "                    if lastCorrectType in correctChunk:\n",
    "                        correctChunk[lastCorrectType] += 1.0\n",
    "                    else:\n",
    "                        correctChunk[lastCorrectType] = 1.0\n",
    "                elif __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) != \\\n",
    "                 __endOfChunk(lastPredTag, predTag, lastPredType, predType) or \\\n",
    "                 (correctType != predType):\n",
    "                    inCorrect = False\n",
    "\n",
    "            if __startOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
    "             __startOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
    "             (correctType == predType):\n",
    "                inCorrect = True\n",
    "\n",
    "            if __startOfChunk(lastCorrectTag, correctTag, lastCorrectType,\n",
    "                              correctType) == True:\n",
    "                foundCorrectCnt += 1\n",
    "                if correctType in foundCorrect:\n",
    "                    foundCorrect[correctType] += 1.0\n",
    "                else:\n",
    "                    foundCorrect[correctType] = 1.0\n",
    "\n",
    "            if __startOfChunk(lastPredTag, predTag, lastPredType,\n",
    "                              predType) == True:\n",
    "                foundPredCnt += 1.0\n",
    "                if predType in foundPred:\n",
    "                    foundPred[predType] += 1.0\n",
    "                else:\n",
    "                    foundPred[predType] = 1.0\n",
    "\n",
    "            if correctTag == predTag and correctType == predType:\n",
    "                correctTags += 1.0\n",
    "\n",
    "            tokenCount += 1.0\n",
    "\n",
    "            lastCorrectTag = correctTag\n",
    "            lastCorrectType = correctType\n",
    "            lastPredTag = predTag\n",
    "            lastPredType = predType\n",
    "\n",
    "        if inCorrect == True:\n",
    "            correctChunkCnt += 1.0\n",
    "            if lastCorrectType in correctChunk:\n",
    "                correctChunk[lastCorrectType] += 1.0\n",
    "            else:\n",
    "                correctChunk[lastCorrectType] = 1.0\n",
    "\n",
    "    if foundPredCnt > 0:\n",
    "        precision = 1.0 * correctChunkCnt / foundPredCnt\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    if foundCorrectCnt > 0:\n",
    "        recall = 1.0 * correctChunkCnt / foundCorrectCnt\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if (precision + recall) > 0:\n",
    "        f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def semantic_acc(pred_slot, real_slot, pred_intent, real_intent):\n",
    "    \"\"\"\n",
    "\tCompute the accuracy based on the whole predictions of\n",
    "\tgiven sentence, including slot and intent.\n",
    "\t\"\"\"\n",
    "    total_count, correct_count = 0.0, 0.0\n",
    "    for p_slot, r_slot, p_intent, r_intent in zip(pred_slot, real_slot,\n",
    "                                                  pred_intent, real_intent):\n",
    "\n",
    "        if p_slot == r_slot and set(p_intent) == set(r_intent):\n",
    "            correct_count += 1.0\n",
    "        total_count += 1.0\n",
    "\n",
    "    return 1.0 * correct_count / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-10 10:36:12,479] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig\n",
    "from langchain import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 加载训练、验证和测试数据\n",
    "data_files = {\n",
    "    \"train\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/train.json\",\n",
    "    \"validation\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/dev.json\",\n",
    "    \"test\": \"/home/shangjian/code/Research/Multimodal & LLM/SLM/data/MixATIS_clean/test.json\"\n",
    "}\n",
    "\n",
    "test_dataset =  load_dataset('json', data_files=data_files['test'])\n",
    "\n",
    "\n",
    "test_template =  \"\"\"\n",
    "[Instruction]\n",
    "you are an expert of spoken language understanding, I need you to perform intent detection and slot filling for given utterance. \\n\n",
    "\n",
    "[Input]\n",
    "utterance: {utterance} \n",
    "\n",
    "[Response]\n",
    "intent: {intent}\n",
    "entity_slot: {entity_slots}\n",
    "\"\"\"\n",
    "\n",
    "test_prompt = PromptTemplate(template=test_template, input_variables=['utterance' 'intent' 'entity_slots'])\n",
    "\n",
    "def format_text(example, is_train=True):\n",
    "    if is_train:\n",
    "        return train_prompt.format(utterance=example['utterance'], \n",
    "                                #    sub_sentence={k: v for k, v in example['sub_utterance'].items() if v is not None},\n",
    "                                   intent=example['intent(s)'],\n",
    "                                   entity_slots={k: v for k, v in example['entity_slots'].items() if v is not None})\n",
    "    else:\n",
    "        return test_prompt.format(utterance=example['utterance'], \n",
    "                                  intent=example['intent(s)'],\n",
    "                                  entity_slots={k: v for k, v in example['entity_slots'].items() if v is not None})\n",
    "\n",
    "# 应用format_text到数据集\n",
    "test_dataset = test_dataset.map(lambda x: {\"formatted_text\": format_text(x, is_train=False)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17274fcebf2c4183b71c8af5535d001f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/414 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/shangjian/miniconda3/envs/pro/lib/python3.8/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Processing:   1%|▏         | 6/414 [04:44<5:12:42, 45.99s/it]"
     ]
    }
   ],
   "source": [
    "#  Restart kneral \n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM \n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_id = '/home/shangjian/code/Research/Multimodal & LLM/dataroot/models/Mistral/Mistral-7B-Instruct-v0.1'\n",
    "peft_path = \"/home/shangjian/code/Research/Multimodal & LLM/SLM/save/model/\" + model_id.split('/')[-1] \n",
    "# peft_path = '/data540/shangjian/Uni-MIS/llm/save/SFT/Mistral-7B-Instruct-v0.1/checkpoint-500'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=1, # beam search\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# loading peft weight\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = model.bfloat16()\n",
    "model.eval()\n",
    "\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 生成回复并计算评估指标\n",
    "all_pred_intents = []\n",
    "all_true_intents = []\n",
    "all_pred_slots = []\n",
    "all_true_slots = []\n",
    "\n",
    "test_template =  \"\"\"\n",
    "[Instruction]\n",
    "you are an expert of spoken language understanding, I need you to perform intent detection and slot filling for given utterance. \\n\n",
    "\n",
    "[Input]\n",
    "utterance: {utterance} \n",
    "\n",
    "[Response]\n",
    "\n",
    "\"\"\"\n",
    "infer_batch_size = 2\n",
    "texts = []\n",
    "for i in range(0,len(test_dataset[\"train\"])):\n",
    "    texts.append(test_template.format(utterance=test_dataset[\"train\"][i][\"utterance\"]))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset[\"train\"]), infer_batch_size), desc=\"Processing\"):\n",
    "        \n",
    "        prompts  = texts[i:i + infer_batch_size]\n",
    "        \n",
    "        model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        generation_outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            max_length=256,  # 或其他适当的最大长度\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_sequences = generation_outputs.sequences.cpu()\n",
    "        \n",
    "        for idx, output in enumerate(tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)):\n",
    "            generated_text = output\n",
    "            # 解析生成的文本以获取预测的意图和槽位\n",
    "            # print(generated_text)\n",
    "            pred_intents, pred_slots, _ = parse_generated_text(generated_text)\n",
    "            \n",
    "            # print(\"--------------------\",pred_intents,pred_slots)\n",
    "            \n",
    "            true_intent, true_slots, utterance = parse_generated_text(test_dataset[\"train\"][i+idx]['formatted_text'])\n",
    "            pred_bio_slots = convert_dict_to_slots(pred_slots, utterance)\n",
    "\n",
    "            # 将真实的entity_slots转换为BIO格式\n",
    "            true_bio_slots = convert_dict_to_slots(true_slots,utterance)\n",
    "\n",
    "            # 添加预测和真实的意图和槽位到列表\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            all_true_intents.append(true_intent)\n",
    "            all_pred_slots.append(pred_bio_slots)\n",
    "            all_true_slots.append(true_bio_slots)\n",
    "        \n",
    "        \n",
    "# 计算多意图准确率、槽位F1分数和语义准确率\n",
    "# 使用之前定义的 get_multi_acc, computeF1Score 和 semantic_acc 函数\n",
    "intent_acc = get_multi_acc(all_pred_intents, all_true_intents)\n",
    "slot_score = computeF1Score(all_true_slots, all_pred_slots)\n",
    "semantic_accuracy = semantic_acc(all_pred_slots, all_true_slots, all_pred_intents, all_true_intents)\n",
    "\n",
    "# 打印评估指标\n",
    "print(f\"Intent Accuracy: {intent_acc}\")\n",
    "print(f\"Slot_Score(f1, precision, recall): {slot_score}\")\n",
    "print(f\"Semantic Accuracy: {semantic_accuracy}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generated_text(generated_text):\n",
    "    # 使用正则表达式匹配意图和实体槽位\n",
    "    intent_pattern = r\"intent\\(s\\): ([\\w#]+)\"\n",
    "    entity_slots_pattern = r\"entity_slots: \\{([^}]+)\\}\"\n",
    "    utterance_pattern = r\"utterance: (.+)\"\n",
    "\n",
    "    # 提取意图\n",
    "    intent_match = re.search(intent_pattern, generated_text)\n",
    "    intents = intent_match.group(1).split('#') if intent_match else []\n",
    "\n",
    "    # 提取实体槽位\n",
    "    entity_slots_match = re.search(entity_slots_pattern, generated_text)\n",
    "    entity_slots = {}\n",
    "    if entity_slots_match:\n",
    "        slots_str = entity_slots_match.group(1)\n",
    "        for slot_str in slots_str.split(', '):\n",
    "            if ':' in slot_str:\n",
    "                key, value = slot_str.split(': ')\n",
    "                entity_slots[key.strip(\"'\")] = value.strip(\"'\")\n",
    "\n",
    "    # 提取utterance\n",
    "    utterance_match = re.search(utterance_pattern, generated_text)\n",
    "    utterance = utterance_match.group(1).strip() if utterance_match else \"\"\n",
    "\n",
    "    return intents, entity_slots, utterance\n",
    "\n",
    "true_intent, true_slots, utterance = parse_generated_text(test_dataset[\"train\"][1]['formatted_text'])\n",
    "\n",
    "# print(test_dataset[\"train\"][1]['formatted_text'])\n",
    "print(true_intent)\n",
    "\n",
    "# 将真实的entity_slots转换为BIO格式\n",
    "\n",
    "# intent_acc = get_multi_acc(all_pred_intents, all_true_intents)\n",
    "# slot_score = computeF1Score(all_true_slots, all_pred_slots)\n",
    "# semantic_accuracy = semantic_acc(all_pred_slots, all_true_slots, all_pred_intents, all_true_intents)\n",
    "\n",
    "# # 打印评估指标\n",
    "# print(f\"Intent Accuracy: {intent_acc}\")\n",
    "# print(f\"Slot_Score(f1, precision, recall): {slot_score}\")\n",
    "# print(f\"Semantic Accuracy: {semantic_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
